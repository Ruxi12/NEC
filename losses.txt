-- 50 epoci, Adam, lr = 0.0001, early stopping not used 
Epoch: 0 Loss: 38315.30650729468
Epoch: 1 Loss: 29920.28019192432
Epoch: 2 Loss: 27682.622272387707
Epoch: 3 Loss: 26169.252261004523
Epoch: 4 Loss: 25298.945729873
Epoch: 5 Loss: 24494.498146734102
Epoch: 6 Loss: 23530.941605169315
Epoch: 7 Loss: 22929.418177851338
Epoch: 8 Loss: 22388.263137740505
Epoch: 9 Loss: 21670.18435325306
Epoch: 10 Loss: 21276.812514936155
Epoch: 11 Loss: 20885.547315076685
Epoch: 12 Loss: 20334.889764272484
Epoch: 13 Loss: 19905.85133124471
Epoch: 14 Loss: 19546.63794317129
Epoch: 15 Loss: 19192.1132252766
Epoch: 16 Loss: 18856.445271637174
Epoch: 17 Loss: 18448.292620303757
Epoch: 18 Loss: 18329.543141658065
Epoch: 19 Loss: 17664.967495279965
Epoch: 20 Loss: 17456.454606064042
Epoch: 21 Loss: 17347.710307035224
Epoch: 22 Loss: 17112.58065650919
Epoch: 23 Loss: 16681.807479475538
Epoch: 24 Loss: 16575.993103072982
Epoch: 25 Loss: 16017.624982808773
Epoch: 26 Loss: 15966.918250897766
Epoch: 27 Loss: 15655.705913375114
Epoch: 28 Loss: 15419.319745627885
Epoch: 29 Loss: 15052.951667259114
Epoch: 30 Loss: 14911.0850957488
Epoch: 31 Loss: 14723.265196859602
Epoch: 32 Loss: 14611.250842238209
Epoch: 33 Loss: 14263.277236881091
Epoch: 34 Loss: 13970.565987682003
Epoch: 35 Loss: 13985.820549447275
Epoch: 36 Loss: 13662.75641789741
Epoch: 37 Loss: 13649.401122583
Epoch: 38 Loss: 13287.36404488205
Epoch: 39 Loss: 13094.584304388552
Epoch: 40 Loss: 13001.290594368269
Epoch: 41 Loss: 12866.015410168262
Epoch: 42 Loss: 12683.391752263868
Epoch: 43 Loss: 12454.986947541023
Epoch: 44 Loss: 12327.085209383427
Epoch: 45 Loss: 12199.770829117439
Epoch: 46 Loss: 11897.159096626108
Epoch: 47 Loss: 11889.722475895373
Epoch: 48 Loss: 11550.135709240867
Epoch: 49 Loss: 11435.024568247647
Precision: 1.0
Recall: 1.0
F1-score: 1.0
Precision: 0.875
Recall: 0.9090909090909092
F1-score: 0.8913043478260869
Precision: 0.8796296296296297
Recall: 0.9393939393939394
F1-score: 0.9079283887468031
Precision: 0.9097222222222222
Recall: 0.8920454545454546
F1-score: 0.8952320058458166
Precision: 0.9277777777777778
Recall: 0.9136363636363637
F1-score: 0.9161856046766534
Precision: 0.8842592592592594
Recall: 0.8724747474747475
F1-score: 0.8745991150083223
Precision: 0.9007936507936509
Recall: 0.8906926406926408
F1-score: 0.8925135271499905
Precision: 0.8715277777777779
Recall: 0.9043560606060607
F1-score: 0.8809493362562417
Precision: 0.8858024691358026
Recall: 0.9149831649831649
F1-score: 0.894177187783326
Precision: 0.8543650793650794
Recall: 0.890151515151515
F1-score: 0.8662979305434548
Precision: 0.8524531024531026
Recall: 0.8849862258953167
F1-score: 0.8633011489788984
Precision: 0.8647486772486773
Recall: 0.8945707070707071
F1-score: 0.8746927198973236




-- 20 de iteratii optimizer = Adam(learn_rate=0.001)
        num_epochs = 20
        dropout_rate = 0.2  # Starting dropout rate
        batch_size = 1.0  #
Iteration: 0  Losses: {'ner': 32130.497174617765}
Iteration: 1  Losses: {'ner': 26422.33606710411}
Iteration: 2  Losses: {'ner': 23648.060757928444}
Iteration: 3  Losses: {'ner': 21348.72044798884}
Iteration: 4  Losses: {'ner': 19866.1426787204}
Iteration: 5  Losses: {'ner': 18105.601612591734}
Iteration: 6  Losses: {'ner': 16625.222168629625}
Iteration: 7  Losses: {'ner': 15226.257765910943}
Iteration: 8  Losses: {'ner': 14323.369816150647}
Iteration: 9  Losses: {'ner': 13106.585383394055}
Iteration: 10  Losses: {'ner': 11972.72928144918}
Iteration: 11  Losses: {'ner': 10934.097241434583}
Iteration: 12  Losses: {'ner': 9990.992995560302}
Iteration: 13  Losses: {'ner': 8914.479654786937}
Iteration: 14  Losses: {'ner': 8386.51805605448}
Iteration: 15  Losses: {'ner': 7364.79271582385}
Iteration: 16  Losses: {'ner': 6689.049813197348}
Iteration: 17  Losses: {'ner': 5831.4187722949855}
Iteration: 18  Losses: {'ner': 5247.3340155659}
Iteration: 19  Losses: {'ner': 4499.133023862228}
Precision:  0.8058111380145279
Recall:  0.8220328516734593
F1-score:  0.8138411689185059


-- 10 iteratii, am adaugat vreo 30 de texte extra generate cu chat gpt
Iteration: 0  Losses: {'ner': 39700.76822417689}
Iteration: 1  Losses: {'ner': 25502.544942728447}
Iteration: 2  Losses: {'ner': 20647.652405833356}
Iteration: 3  Losses: {'ner': 17778.27398100447}
Iteration: 4  Losses: {'ner': 15481.121648765595}
Iteration: 5  Losses: {'ner': 14038.953011349613}
Iteration: 6  Losses: {'ner': 12248.650858494453}
Iteration: 7  Losses: {'ner': 10845.953258798612}
Iteration: 8  Losses: {'ner': 9800.787674067204}
Iteration: 9  Losses: {'ner': 8892.884621271558}
Precision:  0.8089559503528839
Recall:  0.821044831419044
F1-score:  0.8149555623659209



-- 20 de epoci de antrenare -> loss-ul scade, insa recall, F1 sunt mai mici 

Iteration: 0  Losses: {'ner': 39821.17529298356}
Iteration: 1  Losses: {'ner': 25257.3296721348}
Iteration: 2  Losses: {'ner': 20778.369161981587}
Iteration: 3  Losses: {'ner': 17860.386915293366}
Iteration: 4  Losses: {'ner': 15538.547767683991}
Iteration: 5  Losses: {'ner': 13747.331818669423}
Iteration: 6  Losses: {'ner': 12108.499821980575}
Iteration: 7  Losses: {'ner': 10886.137104333455}
Iteration: 8  Losses: {'ner': 9547.009949009833}
Iteration: 9  Losses: {'ner': 8868.333231482182}
Iteration: 10  Losses: {'ner': 8130.653817959614}
Iteration: 11  Losses: {'ner': 7407.418789222871}
Iteration: 12  Losses: {'ner': 7050.551252896329}
Iteration: 13  Losses: {'ner': 6670.532749806565}
Iteration: 14  Losses: {'ner': 6075.476736302982}
Iteration: 15  Losses: {'ner': 5582.398344838041}
Iteration: 16  Losses: {'ner': 5288.01906713974}
Iteration: 17  Losses: {'ner': 5000.021254771354}
Iteration: 18  Losses: {'ner': 4849.0924259605445}
Iteration: 19  Losses: {'ner': 4487.990819142679}
Precision:  0.8023566569484937
Recall:  0.8157342225515624
F1-score:  0.8089901402412886



-- am adaugat partea de scor 

Iteration: 0  Losses: {'ner': 39013.6157016868}
Iteration: 1  Losses: {'ner': 24867.1017562274}
Iteration: 2  Losses: {'ner': 20547.603219295874}
Iteration: 3  Losses: {'ner': 17635.69722751665}
Iteration: 4  Losses: {'ner': 15062.28127155799}
Iteration: 5  Losses: {'ner': 13388.176244331155}
Iteration: 6  Losses: {'ner': 11890.789665508148}
Iteration: 7  Losses: {'ner': 10541.517491089444}
Iteration: 8  Losses: {'ner': 9559.91523751775}
Iteration: 9  Losses: {'ner': 8651.50327517148}

Precision:  0.8039429124334785
Recall:  0.8209213288872422
F1-score:  0.8123434158264589




Iteration: 0  Losses: {'ner': 40281.09729158353}
Iteration: 1  Losses: {'ner': 25050.549921328176}
Iteration: 2  Losses: {'ner': 20759.417950281706}
Iteration: 3  Losses: {'ner': 17437.52418812582}
Iteration: 4  Losses: {'ner': 14988.077912482693}
Iteration: 5  Losses: {'ner': 13196.931388790168}
Iteration: 6  Losses: {'ner': 11909.342853853024}
Iteration: 7  Losses: {'ner': 10597.919986218549}
Iteration: 8  Losses: {'ner': 9657.04764285032}
Iteration: 9  Losses: {'ner': 8632.717328139952}
you are a bot that receives a raw text in Romanian language and returns entity information in a structured manner. For example for this text:
'Început de octombrie, zi minunată de toamnă cu elevi frumoși dornici de lectură!Doamna profesor Radu Cătălina împreună cu un grup de elevi de la Colegiul Tehnic ,,Paul Dimo” au vizitat Filiala nr. 2 ,,Paul Păltănea” pentru a afla mai multe informații despre serviciile pe care biblioteca le oferă tuturor utilizatorilor.'
The answer should be:
{'entities': [(0, 20, 'DATETIME'), (37, 43, 'DATETIME'), (47, 52, 'PERSON'), (80, 109, 'PERSON'), (133, 138, 'PERSON'), (145, 173, 'ORG'), (185, 215, 'FACILITY'), (305, 319, 'PERSON')]})

Please generate the answer for this text:
'Joi, postul Al-Jazeera a difuzat apelul pe care liderul Consiliului Înțelepților Musulmani, Harith al Dhari, l-a făcut la răpitori pentru a-i elibera pe cei patru ostatici, spunând că o astfel de răpire aruncă Irakul și lumea arabă într-o lumină nefastă.'
